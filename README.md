#Twitter Interest Classification#
A classifier to predict the interest of any Twitter user, using basic NLP techniques. 

##Setup##
Install required packages:
```
python dependencies.py
```
Run the script:
```
python classifier.py
```

##Explanation##
In this study, we shall attempt to write a Python script to determine the interest of Twitter users using text classification techniques. Ten categories of interests were chosen such that each category was distinctly different from another. Of these ten, 5 Twitter users of each category was identified on the assumption that all of the tweets from that user were entirely representative of the interest category. The Twitter users were carefully considered to ensure that their tweets were in line with the interest we have assigned to them. For example, one might assume that Lady Gaga's tweets are representative of someone interested in music. Upon closer inspection, we find that her timeline is filled with tweets of a more personal nature, and few are actually related to music. Hence, if a classifier were to be trained using such unrepresentative data, the predictions obtained would be highly inaccurate and any form of feature engineering would likewise be less effective.

With this understanding in mind, a total of 50 Twitter users were selected as training examples. A Python script was written to obtain the latest 3,200 tweets of each user through the Tweepy library, which makes direct API calls to the Twitter server. As each request was limited to 200 tweets, 16 requests were necessary to obtain the Twitter maximum of 3,200 tweets per user. In order to stay within the stipulated rate limits, the script cycled through a collection of 5 API keys to stay within the stipulated rate limits. Needless to say, each user had to have a minimum of 3,200 tweets in their timeline to be considered in our model. This gives us a total of approximately 154,248 tweets in our sample dataset, after removing invalid requests.

####Preprocessing and Feature Extraction####
A quick inspection of our dataset revealed that a large number of tweets contained URLs, and this was confirmed with a regular expression query that counts the number of such tweets. It was found that 129,116 of the 154,248 (83.7%) total tweets contained URLs. This could potentially result in highly correlated features, which could in turn reduce the efficacy of conditional probability-based algorithms such as Naive Bayes. As such, all URLs were removed from the dataset. Thereafter, punctuation and numerical digits were removed, and all tweets were converted to lowercase as these features provide little value in predicting the interest of a tweet, and could reduce the overall accuracy by increasing the dimensionality of the feature vector.

An initial feature vector was then obtained with n-gram of size 1, using the porter stemming algorithm, removal of stopwords, inclusion of inverse document frequency score (TFIDF), and no feature selection. We shall now refer to classifiers trained using a feature vector with these settings to be the baseline. In order to investigate how best to extract the features, a Naive Bayes algorithm was chosen, due to its fast training time, to train the baseline classifier. In order to evaluate the feature vectors, we will be using 3 measures. First, the mean of a k-fold cross validation (k=10). This helps us determine the percentage of correct predictions that are being made using the classification model built using the specified feature vector. Second, the F-score calculated using the predictions of the trained classifier. This is the weighted average of Precision (P), or the percentage of selected items that are correctly predicted, and Recall (R), or the percentage of correct items that are selected. The F-score is bounded by (0,1) with 1 being the best and 0 the worst. Third, the confusion matrix, which allows us to identify the categories that the algorithm tends to get confused by. Correct predictions are observed in the diagonal of the square matrix, and any significant deviations from the diagonal indicate that the classifier is having trouble differentiating between certain inputs. The results of the various feature vectors will only be briefly touched upon in this section, with deeper analysis of our optimal classifier in the results section.

#####Baseline Feature Vector#####
The baseline feature vector contained 88,459 features with an average k-fold score of 87.7% and average F-score of 0.91. The classifier tends to confuse news, politics, and finance as observed in the confusion matrix, but the bulk of the predictions still lie in the diagonal. This may be due to the fact that people that are interested in politics and finance tend to tweet about recent news headlines, given the currency of those topics. However, the number of tweets that are classified wrongly is much less than those classified correctly. In this instance, 598 tweets were classified as news instead of politics, while 10,821 tweets were correctly classified as politics.

#####Varying the Size of the n-gram#####
The size of the n-gram in the baseline was 1, meaning that each feature consists of a unigram. The size of the n-gram can be increased to include bigrams and trigrams, which will result in a larger feature vector and helps to inject some context. For example, a feature vector consisting of unigrams will count the frequency (or inverse document frequency) of the word “happy”. Conversely, a feature vector consisting of bigrams could give us more details for features like “not happy” or “very happy”, which gives us more insights into the general sentiment of a tweet.

The feature vector with n-gram of size 2 contained 814,440 features with an average k-fold score of 89.4% and average F-score of 0.96. The classifier seems to have less difficulty differentiating between categories such as news, politics, and finance as compared to the baseline. When bigrams are included as features, we observe an overall increase in k-fold score and F-score. We then repeat the experiment with n-gram of size 3. The feature vector contained 1,715,832 features with an average k-fold score of 89.2% and average F-score of 0.98. The results obtained from the confusion matrix has continued to improve as compared to its predecessor. The inclusion of trigrams seems to have caused a slight decrease in k-fold score, and a slight increase in F-score. To avoid overfitting our model, subsequent n-gram sizes will not be taken into consideration.

#####Varying the Stemming Algorithm used#####
The baseline feature vector used the Porter stemming algorithm, which is known to be a gentle algorithm. It would be interesting to observe the behavior of the classifier when a more aggressive algorithm, such as the Snowball algorithm, is applied to each token. This will have a greater effect in reducing the size of the feature vector at the expense of lesser distinction between tokens. The feature vector using the Snowball algorithm contained 88,639 features with an average k-fold score of 87.7% and average F-score of 0.91. The experiment is repeated with no stemming algorithms applied. The resultant feature vector contained 108,380 features with an average k-fold score of 88% and average F-score of 0.92. There is a marginal improvement in performance at the expense of an additional 19,741 features. The increased dimensionality may introduce complications as the study progresses, and contributes to increased training time of the classifiers. As such, the baseline feature vector has the best performance and we will stick with the Porter stemmer.

#####Presence of Stopwords#####
The list of stopwords used in the baseline feature vector was obtained from the Python Natural Language Toolkit (NLTK), and consists of 127 unique stopwords. They are typically removed from a document prior to any text classification tasks, but given that tweets are a significantly shorter form of document (140 characters), the stopwords may have a bigger impact than originally thought. The feature vector with stopwords contained 88,460 features with an average k-fold score of 87.4% and average F-score of 0.91. There is a slight decrease in k-fold score and no change in F-score. Hence, we may safely conclude that the presence of stopwords has little or no impact on the overall performance of the classifier. This may be due to the fact that the baseline feature vector was using the TFIDF score instead of just term frequency, which would register that the stopwords appeared in many documents and thus, are not unique identifiers.

#####Use of Inverse Document Frequency Score#####
Given that tweets are significantly shorter than traditional documents, we may assume to a certain extent that the words being used in a tweet are deliberate and representative of the interest of a tweet. This makes the importance of the IDF all the greater when dealing with tweets, as the identification of a term unique to that interest will boost the performance of the classifier. However, to guard against hasty assumptions, there is a need to analyze the results without the IDF score. The feature vector obtained using just the Term Frequency (TF) score contained 88,459 features with an average k-fold score of 85.8% and average F-score of 0.89. This decrease in k-fold score and F-score confirms our earlier assumptions that a classifier built using the TF-IDF score instead of just TF scores would yield better results.

#####Univariate Feature Selection#####
Feature selection can be a useful tool to reduce the number of features, which theoretically should increase the quality of the selected feature vector while reducing training time. For each feature, a statistical test is conducted to determine the quality of that feature. Here, we explore the performance of the classifiers when the ANOVA F-test and Chi-Squared test are used to find the top 20,000 features. The feature vectors obtained through univariate feature selection using ANOVA and Chi-Squared both have an average k-fold score of 87.5% and average F-score of 0.89. At first glance, this slight decrease in k-fold and F-scores seem to be counterintuitive. If the best features are chosen, the classifier should perform better. However, a deeper analysis of the scenario seems to indicate that although the selected 20,000 features may be better than the unselected features, these unselected features may actually have a significant role to play in predicting the interest of a user. Hence, considering the size of our dataset, it is unsurprising that removing these features reduced the overall performance of our classifier instead. Feature selection may prove more useful when there is an abundance of features that greatly exceed the size of our sample, and actually contribute to a significant increase in training time. One possible improvement could be to conduct feature selection on a dataset with n-gram of size 3. Since the dataset would include unigrams, bigrams, and trigrams, the number of features would be significantly larger. A feature vector with n-gram of size 3 was obtained through univariate feature selection using ANOVA to find the top 200,000 features (out of 1,715,832 features), with an average k-fold score of 90.6% and average F-score of 0.93.

##Results##
While each of the feature extraction methods will alter the way the classifier works, there is little observable significant impact on the overall performance. The average k-fold scores range from 85.8% to 90.6%, and the average F-score ranges from 0.89 to 0.98. The final feature vector was selected because it had a high F-score which implies a good balance between high precision and recall, and a reasonably high k-fold score which ensures that we are not overfitting our model. This final feature vector was similar to the baseline feature vector, except with a n-gram range of 1 to 3 and feature selection using ANOVA. The performance of various classification algorithms were then assessed, including Naive Bayes (NB), Support Vector Machines (SVM) and Logistic Regression (LR). Using the top 200,000 features, the NB model had an average k-fold score of 90.6% and average F-score of 0.93, while the LR model had an average k-fold score of 89.60% and average F-score of 0.93. The SVM model took an incredibly long amount of time, so the top number of features was reduced to 1000. We generated 4 different SVM models with various kernels, and the results are as follows:

Kernel Type | Linear | Polynomial | RBF | Sigmoid
--- | --- | --- | --- | ---
Average k-fold score (%) |	70.5 |	10.5 | 44.3 |	10.5
Average f-score	| 0.72 |	0.02 | 0.52 |	0.02
Training Time (mins) |	14 |	50 |	69 |	42

From the initial results, we can observe that the polynomial and sigmoid kernels are obviously bad fits for the data, and should not be considered for future classification. The RBF kernel shows a slight improvement, but performs more poorly than the linear kernel and requires a much longer training time even with just 1000 features. To improve on the performance of the linear kernel, the number of features could be increased so as to ensure that important features are not removed. The process was repeated with 200,000 features, which still trained slowly so an alternative implementation of the linear SVM kernel was applied, using Stochastic Gradient Descent (SGD) learning instead. This model had an average k-fold score of 87.8% and average F-score of 0.90.

Overall, the best algorithms for this particular classification task is the NB and LR models, given that they both have the highest F-score and their k-fold scores differ by only one percentage point. The SVM, using a linear kernel with SGD learning, is a close second but given its longer training time, will be less useful to us. Eventually, the Naive Bayes classifier was selected because it was the fastest model to be trained (0.37s for a feature vector of size 200,000). The Logistic Regression model took significantly longer (21.8s for the same feature vector), but may prove useful for future classification tasks because it tends to perform better in datasets with correlated features, in particular larger datasets. A Naive Bayes classifier was trained with the final feature vector, yielding this classification report:

![alt text](http://andretan.sg/images/classif_report.png "Classification report")

As can be observed, the interest with the lowest precision is News. This means that 87% of tweets classified as News are true positives, with 13% wrongly classified. Precision alone, as with Recall, is insufficient for us to make any significant observations given how the two measures are inversely related to each other. The interest with the lowest recall is Politics, and this means that the classifier is able to correctly classify tweets about Politics 87% of the time. We are most interested in the categories with the lowest F-scores, as this represents a weighted average of both precision and recall. News has the lowest F-score of 0.89, but even this value is not significantly dismal. To better understand the categories that the classifier confuses with News, we generate a confusion matrix which allows us to observe the True Positive, True Negative, False Positive and False Negative distributions of the classifier.

![alt text](http://andretan.sg/images/confusion_matrix.png "Confusion matrix")

Most of the predictions are True positives and centered around the diagonal. There is a small portion of tweets that are political in nature, but have been predicted as news and vice versa. However, this proportion is small compared to the rest of the predictions, as can be seen in the color scale. The overall performance is largely satisfactory, although the same cannot be said when more interests are added to the model, as the classifier will have greater difficulty differentiating between them.

The average k-fold score and F-scores are insufficient in measuring the overall accuracy of the classifier, as they only indicate how good a classifier is at predicting the data that it was trained with by verifying it with a test dataset or cross validation. As such, a list of Twitter handles were compiled and manually labeled with an interest. A prediction was then made for each Twitter handle, by aggregating all 3 classifiers (NB, SVM and LR) and taking the majority prediction. This would allow us to leverage on the respective strengths of each classification algorithm, and obtain greater overall accuracy which was measured to be the number of correct predictions over the total predictions. In a manually curated list of 31 users, the classifier predicted 26 correctly. Upon closer inspection of the incorrect predictions, it was found that the tweets posted by a user may not necessarily reflect their interests, and that not all interest categories are mutually exclusive. For example, narendramodi is India's current Prime Minister and obviously a politician. Yet, his tweets may have an uplifting tone in order to inspire his followers, and therefore was classified as inspirational. Similarly, CNN and time are accounts from news agencies. However, they were predicted to be interested in politics and tech respectively. This can be attributed to the nature of news that they are reporting on, and similar false predictions may arise when users report news on Finance or Sports etc. It was also found that neymarjr, a prominent soccer superstar, was classified to be interested in Finance. This result is more surprising given how disjoint the two categories seemed to be. Closer inspection of the results revealed that neymarjr's tweets were not in English, which explains why the classifier did not know how to interpret his tweets given the lack of relevant training data.

##Conclusions and Future Improvements##
One key takeaway from this study is the importance of selecting good training data. A classifier will only be as good as the data it learns from, so one must resist the temptation to just pick the most popular Twitter users in a category, and instead pick out the people who talk and discuss about a certain interest such that their tweets are representative of said interest. Even then, there are certain limitations to the current, albeit convenient, methodology of obtaining training data, where all of the tweets by a user can be assumed to be of a similar interest. This is because people tend to have multiple interests, and someone who is interested in music may not always tweet about music. Hence, the classifier must be able to provide multiple predictions of interest, and even be able to state that it is unsure about a certain prediction. When a classifier is unsure, this means it is having difficulty determining the exact interest of a person, possibly because there isn't just one interest. In these scenarios, the classifier might end up making a wrong prediction, in part due to the fact that our list of categories are non-exhaustive. By teaching the classifier to state the ambiguous cases, we can isolate and manually study the tweets responsible, before improving on the classification algorithm iteratively.
