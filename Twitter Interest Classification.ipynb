{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Interest Classification with Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this study, we shall attempt to write a Python script to determine the interest of Twitter users using text classification techniques. As a proof-of-concept, we shall first assume that all users only have one interest and proceed from there. The approach that we shall be adopting will be one of supervised learning, which implies that we required labelled training data. 10 broad categories have been chosen to represent the interests of an individual; fashion, finance, food, gaming, inspiration, music, news, politics, sports, technology. For each category, 5 Twitter users were chosen that tweet regularly about that respective category. For example, one of the Twitter users we selected for the News category was \"BBCBreaking\", a channel for BBC News to post tweets on breaking news. We are basically obtaining training data by mining tweets from prominent accounts and implicitly labelling these tweets with a predefined interest category. \n",
    "\n",
    "With this understanding in mind, a total of 50 Twitter users were selected (as depicted in the table\n",
    "below). A Python script was written to obtain the latest 3,200 tweets of each user through the Tweepy\n",
    "library, which makes direct API calls to the Twitter server. As each request was limited to 200 tweets, 16\n",
    "requests were necessary to obtain the Twitter maximum of 3,200 tweets per user. In order to stay within\n",
    "the stipulated rate limits, the script cycled through a collection of 5 API keys to stay within the stipulated\n",
    "rate limits. Needless to say, each user had to have a minimum of 3,200 tweets in their timeline to be\n",
    "considered in our model. This gives us a total of approximately 154,248 tweets in our sample dataset,\n",
    "after removing invalid requests. \n",
    "\n",
    "Let's load the required dependencies and our initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas,csv,re,random,tweepy,datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "from accesstokenTwitter import * # python file containing a list of access tokens\n",
    "from collections import Counter\n",
    "\n",
    "test_data = pandas.read_csv('test_data.csv') # csv file with 2 headers: name, interest\n",
    "Count_Vectorizer, Tfidf_Transformer, starttime, cachedStopwords, myStopwords, snowball, porter, selector, lancaster = None, None, None, None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
